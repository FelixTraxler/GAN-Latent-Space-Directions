{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "from ft_utils.BatchImageGenerator import BatchImageGenerator\n",
    "from ft_utils.BatchImageClassifier import BatchImageClassifier\n",
    "from ft_utils.utils import BATCH_SIZE\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = [\"blonde\", \"not blonde\"]\n",
    "\n",
    "batch_classifier = BatchImageClassifier(\"out_batch_transfer\")\n",
    "batch_generator = BatchImageGenerator(\"out_batch_transfer\", True)\n",
    "\n",
    "scores = []\n",
    "latent_vectors_list = []\n",
    "\n",
    "text_features = batch_classifier.tokenize_attributes(attributes)\n",
    "\n",
    "print(\"Scoring images\")\n",
    "for i in tqdm(range(0, round(200_000 / BATCH_SIZE))):\n",
    "    probs = batch_classifier.classify_from_batch(i*BATCH_SIZE, BATCH_SIZE, text_features)\n",
    "    scores.extend([t[0,0].item() for t in probs]) # Use extend for efficiency\n",
    "    latent_vectors_list.append(batch_generator.load_w_batch(i*BATCH_SIZE, BATCH_SIZE))\n",
    "\n",
    "latent_vectors = np.concatenate(latent_vectors_list, axis=0)\n",
    "scores = np.array(scores).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ft_utils.InterfaceGANMethod\n",
    "import ft_utils.AverageMethod\n",
    "import ft_utils.GaussianMethod\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ft_utils.InterfaceGANMethod)\n",
    "importlib.reload(ft_utils.AverageMethod)\n",
    "importlib.reload(ft_utils.GaussianMethod)\n",
    "\n",
    "methods = {\n",
    "    \"InterfaceGAN\": ft_utils.InterfaceGANMethod.InterfaceGANMethod(),\n",
    "    \"Average\": ft_utils.AverageMethod.AverageMethod(),\n",
    "    \"Gaussian\": ft_utils.GaussianMethod.GaussianMethod()\n",
    "}\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for method_name, method in methods.items():\n",
    "    print(\"{}: Training...\".format(method_name))\n",
    "    method.train(latent_vectors, scores)\n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Measuring...\")\n",
    "\n",
    "method_scores = {\n",
    "    \"InterfaceGAN\": {\n",
    "        \"openclip_scores\": [],\n",
    "    },\n",
    "    \"Average\": {\n",
    "        \"openclip_scores\": [],\n",
    "    },\n",
    "    \"Gaussian\": {\n",
    "        \"openclip_scores\": [],\n",
    "    },\n",
    "    \"Original\": {\n",
    "        \"openclip_scores\": [],\n",
    "    }\n",
    "}\n",
    "\n",
    "def measure_seed(initial_w_vector):\n",
    "    ws_first = torch.from_numpy(initial_w_vector.astype(np.float32)[np.newaxis, :].repeat(14, axis=1)).to(\"mps\")\n",
    "    original_image = batch_generator.generate_from_w_vec(ws_first[0], filename=None)\n",
    "    original_scores = batch_classifier.classify_image_vec(original_image, text_features)\n",
    "    # if original_scores[0][1] < 0.3: return\n",
    "\n",
    "    method_scores[\"Original\"][\"openclip_scores\"].append(original_scores.detach().numpy())\n",
    "\n",
    "    for method_name, method in methods.items():\n",
    "        resulting_w_vector = method.latent_walk(initial_w_vector)\n",
    "        ws = torch.from_numpy(resulting_w_vector.astype(np.float32)[np.newaxis, :].repeat(14, axis=1)).to(\"mps\")\n",
    "        image = batch_generator.generate_from_w_vec(ws[0], filename=None)\n",
    "        new_scores = batch_classifier.classify_image_vec(image, text_features)\n",
    "        method_scores[method_name][\"openclip_scores\"].append(new_scores.detach().numpy())\n",
    "\n",
    "batch_idx_start = 1 + round(200_000 / BATCH_SIZE)\n",
    "batch_idx = batch_idx_start\n",
    "seed_idx = 0\n",
    "num_samples = 100\n",
    "\n",
    "current_sample_group = batch_generator.load_w_batch(batch_idx*BATCH_SIZE, BATCH_SIZE)\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    if seed_idx >= 64:\n",
    "        seed_idx = 0\n",
    "        batch_idx += 1\n",
    "        current_sample_group = batch_generator.load_w_batch(batch_idx*BATCH_SIZE, BATCH_SIZE)\n",
    "        print(f\"Batch {batch_idx - batch_idx_start + 1}/{num_samples/64:.0f}\")\n",
    "    measure_seed(current_sample_group[seed_idx:seed_idx+1])\n",
    "    seed_idx += 1\n",
    "\n",
    "og_scores = method_scores[\"Original\"][\"openclip_scores\"]\n",
    "og_avg = np.average([score[0, 0] for score in og_scores])\n",
    "\n",
    "for method_name, method in methods.items():\n",
    "    scores = method_scores[method_name][\"openclip_scores\"]\n",
    "    avg = np.average([score[0, 0] for score in scores])\n",
    "    print(f\"{method_name}: score improvement {100*(avg - og_avg):+.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
